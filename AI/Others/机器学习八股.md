# 机器学习八股

## softmax指数上溢

1. softmax函数定义： $softmax(x_i) = exp(x_i) / Σ_j exp(x_j)$
2. 指数上溢问题：输入向量x_i过大导致超出浮点数所能表示的范围，导致数值溢出。这会使计算结果变为NaN或Inf。
3. 为什么会发生：在深度神经网络中，尤其是网络很深或者权重初始化不当时，中间层的输出可能会变得非常大。

一个简单有效的解决方案是：在计算exp之前，从每个输入中减去输入向量的最大值。这不会改变softmax的结果，但可以有效防止上溢。

## Transformer中的positional encoding

### 为什么？

在Transformer中，我们使用自注意力机制来处理输入序列。然而，自注意力本身是“位置无关”的--它不考虑输入元素的顺序。对于NLP任务来讲，词序顺序至关重要。因此，我们需要某种方式来将位置信息注入模型。

### 基本思想

基本思想是为输入序列的每个位置创建一个唯一的向量表示，然后将这个向量驾到相应位置的词嵌入上。这样，模型就能“感知”到每个词在序列中的相对位置。

### Sinusoidal Positional Encoding

Transformer原始论文中使用的是正弦和余弦的组合来生成位置编码。这种方法的优点是可以处理任意长度的序列，即使是训练集中没有出现过的长度。

### 公式

对于位置$pos$和维度$i$:

$PE_{pos, 2i} = sin(pos / 10000^{2i / d_{model}})$
$PE_{pos, 2i+1} = cos(pos / 10000^{2i / d_{model}})$

其中：

- $pos$是词在序列中的位置
- $i$是为度的索引
- $d_{model}$是模型的维度

特点：

- 每个位置的编码是唯一的。
- 对于任何固定的偏移量 $k$, $PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数。
- 它允许模型容易地学习关注相对位置。

```python
import numpy as np

def positional_encoding(max_position, d_model):
    position = np.arange(max_position)[:, np.newaxis]
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    pos_encoding = np.zeros((max_position, d_model))
    pos_encoding[:, 0::2] = np.sin(position * div_term)
    pos_encoding[:, 1::2] = np.cos(position * div_term)
    
    return pos_encoding

# 示例使用
max_position = 100
d_model = 512
pe = positional_encoding(max_position, d_model)
```

### Positional Encoding的可视化

如果我们将位置编码可视化，会发现它呈现一种有趣的波浪模式。较低的维度（靠近矩阵顶部的行）变化更快，而较高的维度变化较慢。这种模式使得模型能够学习关注不同尺度的位置关系。

### 其他Positional Encoding方法

除了正弦位置编码，还有其他方法可以引入位置信息：

1. 学习型位置编码：直接学习一个位置嵌入矩阵。
2. 相对位置编码：只考虑词之间的相对距离，而不是绝对位置
3. 旋转位置编码（RoPE）：通过复数旋转来编码相对位置信息。

### 总结

Positional Encoding是Transformer结构中的一个关键创新它巧妙地解决了如何在无序的自注意力机制中引入序列顺序信息的问题。通过使用三角函数，它不仅能够为每个位置提供唯一的表示，还能让模型容易地推断和学习位置之间的关系。

## Batch Norm

一种正则化技术。目的是解决内部协变量偏移（internal covariate shift）问题，从而加速网络训练提高网络性能。

基本思想：

1. 对每一个batch的输入数据进行归一化处理
2. 引入可学习的缩放和平移参数

```plaintxt
μ = 1/m * Σx  (batch mean)
σ² = 1/m * Σ(x - μ)²  (batch variance)
x̂ = (x - μ) / √(σ² + ε)  (normalize)
y = γ * x̂ + β  (scale and shift)
```

其中 γ 和 β 是可学习的参数，ε 是一个很小的常数用于数值稳定性。

### 训练过程中的Batch Norm

1. 对每一个mini-batch计算均值和方差并归一化
2. 缩放和平移（scale and shift）
3. 同时，记录每个batch的均值和方差的移动平均值，用于推理阶段

### 推理过程中的Batch Norm

1. 不再计算batch统计量
2. 使用训练阶段记录的移动平均值作为均值和方差
3. 使用这些固定的统计量进行归一化
4. 应用训练好的缩放和评议参数

### 移动平均值的计算

通过对每个batch的统计量进行加权平均来计算：

```plaintxt
# μ_moving 和 σ²_moving 是移动平均值
# μ_batch 和 σ²_batch 是当前batch的统计量
# momentum 是一个小于1的正数,通常设置为0.9或0.99

μ_moving = momentum * μ_moving + (1 - momentum) * μ_batch
σ²_moving = momentum * σ²_moving + (1 - momentum) * σ²_batch
```

这种计算方式有以下特点：

- 赋予最近的batch更高的权重
- 随着训练进行，earlier batches的影响逐渐减小

### scale and shift 参数学习

缩放和平移参数是通过反向传播与网络的其他参数一起学习的。过程如下：

1. 初始化: γ通常初始化为1，β通常初始化为0
2. 前向传播: 应用缩放和平移: y = γ x̂ + β
3. 反向传播: ∂L/∂γ = Σ(∂L/∂y * x̂), ∂L/∂β = Σ(∂L/∂y)
4. 参数更新: `γ = γ - learning_rate * ∂L/∂γ, β = β - learning_rate * ∂L/∂β`
