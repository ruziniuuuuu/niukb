# 机器学习基础

## 有监督、无监督、自监督学习

1. 有监督学习：
   - 使用标记好的数据进行训练
   - 每个训练样本都有输入特征和相应的正确输出标签
   - 目标是学习从输入到输出的映射关系
   - 常见任务：分类、回归
2. 无监督学习：
   - 使用未标记的数据进行训练
   - 没有明确的输出标签
   - 目标是发现数据中的内在结构或模式
   - 常见任务：聚类、降维、异常检测
3. 自监督学习：
   - 使用未标记数据，但通过数据本身创建标签
   - 设计一个“伪任务”，从数据中自动生成监督信号
   - 目标是学习有用的特征表示
   - 常见技术：掩码语言建模、图像旋转预测等

## 关于shortcut

深度学习中的shortcut（也称为skip connection或residual connection）允许信息直接从ealier layers跳过一些中间层直接传递到later layers。

### ShortCut的数学表达式

$$ y = F(x) + x $$

其中x是输出，F(x)是经过某些层的变换，y是最终输出。

### ShortCut的作用

1. 缓解梯度消失问题：在深度网络中，梯度可能在反向传播过程中变得非常小。Shortcut提供了一条梯度流动的捷径，有助于训练更深的网络。
2. 允许网络学习恒等印社：如果最优函数接近于恒等映射，残差学习比直接学习该函数更容易。
3. 提高信息流：允许网络更容易地访问早期层的特征。

## 反向传播Back Propagation

反向传播梯度：

- 从输出层开始，计算损失函数对该层输出的梯度
- 利用链式法则，逐层向后传播梯度
- 对每一层，计算：
  - 损失函数对该层输出的梯度
  - 损失函数对该层权重和偏置的梯度
- 参数更新
  - 利用optimizer根据计算得到的梯度更新网络参数

假设一个简单的两层网络：

```plaintxt
输入 x -> 隐藏层 h -> 输出 y
```

前向传播：

h = f(Wx + b), y = g(Vh + c)

其中W, V是权重矩阵，b, c是偏置，f和g是激活函数。

反向传播：

1. 计算输出层梯度: δy = ∂L/∂y g'(Vh + c)
2. 计算隐藏层梯度: δh = V^T δy f'(Wx + b)
3. 计算参数梯度：
   - ∂L/∂V = δy h^T
   - ∂L/∂c = δy
   - ∂L/∂W = δh x^T
   - ∂L/∂b = δh

## 优化器optimizer

optimizer是用来最小化损失函数的算法。它们通过调整模型的参数来降低训练误差，从而提高模型的性能。
