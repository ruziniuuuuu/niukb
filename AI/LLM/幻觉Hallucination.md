# LLM中的幻觉Hallucination

## Survey of Hallucination in Natural Language Generation

> [为什么大模型会「说胡话」？如何解决大模型的「幻觉」问题？ - 平凡的回答 - 知乎](https://www.zhihu.com/question/635776684/answer/3336439291)

### 什么是幻觉？

> the generated content that is nonsensical or unfaithful to the provided source content

### 幻觉的类型

幻觉主要分为两种类型：

- 内在幻觉：生成的内容与源内容相互矛盾
- 外在幻觉：生成的内容无法从源内容中验证，既可能正确也可能错误

### 幻觉的原因

- 数据驱动原因：训练数据中源与参考的不匹配可能导致幻觉，如数据对不对齐，导致生成不忠实的文本。
- 表示和解码的不完善：编码器理解能力的缺陷和解码器策略错误可能导致幻觉。解码器可能关注错误的输入部分，或使用增加幻觉风险的策略，例如基于采样的解码中的随机性。
- 参数知识偏见：预训练模型可能偏好其参数中的知识而非新输入，从而导致幻觉。

### 解决方法

- 构建忠实数据集
- 自动清洗数据
- 信息增加：用外部信息增强输入数据，更好地与目标对齐，提高模型的语义理解能力
- 模型和推理技术
  - 改进编码器和注意力机制：修改编码器结构和注意力机制，更多关注源数据，提高表示学习。
  - 改进解码器使用多分枝、意识到不确定性或受限的解码器来减少幻觉的可能性
- 训练方法：
  - 规划/草图：实施单独的规划步骤将其整合到模型中，控制内容生成。
  - 强化学习：使用奖励优化模型，以减少幻觉。
  - 多任务学习：同时在多个任务上训练，提高模型的泛化能力，减少对单一数据集的依赖。
- 可控生成：使用技术控制输出中的幻觉成都，适应不同的应用需求。
- 后处理：使用生成-然后-精炼策略纠正输出中的幻觉，特别适用于嘈杂的数据集。

### 未来方向

- 自动化幻觉检测和评估方法
- 跨领域和多模态方法
- 增强现实世界知识
- 解释性和透明性
