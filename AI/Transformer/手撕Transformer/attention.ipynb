{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(128, 32, 512)\n",
    "d_model = 512\n",
    "n_head = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # 存储每个头的维度\n",
    "        self.d_k = d_model // n_head\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch, time, dimension = q.shape\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        # reshape q, k, v\n",
    "        q = q.view(batch, time, self.n_head, self.d_k).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch, time, self.n_head, self.d_k).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch, time, self.n_head, self.d_k).permute(0, 2, 1, 3)\n",
    "        # score\n",
    "        score = q@k.transpose(2, 3)/math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask==0, -10000)\n",
    "        score = self.softmax(score)@v\n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)\n",
    "        outputs = self.w_combine(score)\n",
    "        return outputs\n",
    "    \n",
    "attention = MultiHeadAttention(d_model, n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2157, -0.0139, -0.1372,  ..., -0.1058, -0.0572,  0.0882],\n",
      "         [ 0.2146, -0.0134, -0.1360,  ..., -0.1048, -0.0578,  0.0881],\n",
      "         [ 0.2152, -0.0134, -0.1359,  ..., -0.1054, -0.0571,  0.0874],\n",
      "         ...,\n",
      "         [ 0.2150, -0.0137, -0.1360,  ..., -0.1053, -0.0575,  0.0872],\n",
      "         [ 0.2157, -0.0142, -0.1361,  ..., -0.1046, -0.0571,  0.0880],\n",
      "         [ 0.2150, -0.0141, -0.1362,  ..., -0.1054, -0.0565,  0.0887]],\n",
      "\n",
      "        [[ 0.2465, -0.0294, -0.1742,  ..., -0.1299, -0.0583,  0.1152],\n",
      "         [ 0.2477, -0.0278, -0.1743,  ..., -0.1299, -0.0574,  0.1148],\n",
      "         [ 0.2480, -0.0280, -0.1736,  ..., -0.1305, -0.0583,  0.1155],\n",
      "         ...,\n",
      "         [ 0.2471, -0.0285, -0.1741,  ..., -0.1303, -0.0579,  0.1143],\n",
      "         [ 0.2475, -0.0285, -0.1744,  ..., -0.1312, -0.0586,  0.1151],\n",
      "         [ 0.2477, -0.0278, -0.1732,  ..., -0.1299, -0.0578,  0.1152]],\n",
      "\n",
      "        [[ 0.2451,  0.0038, -0.1495,  ..., -0.0586, -0.0730,  0.0898],\n",
      "         [ 0.2451,  0.0030, -0.1499,  ..., -0.0582, -0.0724,  0.0890],\n",
      "         [ 0.2449,  0.0027, -0.1506,  ..., -0.0580, -0.0734,  0.0887],\n",
      "         ...,\n",
      "         [ 0.2443,  0.0042, -0.1500,  ..., -0.0597, -0.0732,  0.0890],\n",
      "         [ 0.2446,  0.0026, -0.1507,  ..., -0.0582, -0.0730,  0.0885],\n",
      "         [ 0.2448,  0.0043, -0.1493,  ..., -0.0586, -0.0727,  0.0896]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2135, -0.0354, -0.1483,  ..., -0.1510, -0.0644,  0.0867],\n",
      "         [ 0.2142, -0.0352, -0.1476,  ..., -0.1519, -0.0642,  0.0872],\n",
      "         [ 0.2143, -0.0346, -0.1486,  ..., -0.1506, -0.0650,  0.0871],\n",
      "         ...,\n",
      "         [ 0.2138, -0.0333, -0.1489,  ..., -0.1518, -0.0643,  0.0873],\n",
      "         [ 0.2141, -0.0355, -0.1482,  ..., -0.1517, -0.0639,  0.0874],\n",
      "         [ 0.2149, -0.0346, -0.1489,  ..., -0.1524, -0.0648,  0.0873]],\n",
      "\n",
      "        [[ 0.2369, -0.0088, -0.1844,  ..., -0.0847, -0.0585,  0.1004],\n",
      "         [ 0.2362, -0.0087, -0.1839,  ..., -0.0830, -0.0578,  0.1001],\n",
      "         [ 0.2368, -0.0090, -0.1837,  ..., -0.0826, -0.0580,  0.1019],\n",
      "         ...,\n",
      "         [ 0.2363, -0.0088, -0.1833,  ..., -0.0827, -0.0573,  0.1011],\n",
      "         [ 0.2369, -0.0088, -0.1849,  ..., -0.0833, -0.0574,  0.1010],\n",
      "         [ 0.2370, -0.0085, -0.1834,  ..., -0.0824, -0.0573,  0.1003]],\n",
      "\n",
      "        [[ 0.2444, -0.0316, -0.1239,  ..., -0.1428, -0.0750,  0.1177],\n",
      "         [ 0.2443, -0.0308, -0.1241,  ..., -0.1422, -0.0745,  0.1176],\n",
      "         [ 0.2438, -0.0321, -0.1253,  ..., -0.1430, -0.0750,  0.1180],\n",
      "         ...,\n",
      "         [ 0.2438, -0.0317, -0.1244,  ..., -0.1432, -0.0747,  0.1179],\n",
      "         [ 0.2444, -0.0317, -0.1244,  ..., -0.1432, -0.0747,  0.1185],\n",
      "         [ 0.2445, -0.0319, -0.1245,  ..., -0.1426, -0.0746,  0.1177]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = attention(x, x, x)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
