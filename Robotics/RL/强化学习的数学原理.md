# Mathematical Foundation of Reinforcement Learning

> [https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning]

## Overview

- [ ] TODO

### Classification of RL

- AI
  - ML
    - Supervised
    - Unsupervised
    - RL

## Basic Concepts

- state -> State space
- action
- state transition
  - at s_1, take a_2: s_1 -> s_2
  - deterministic
  - stochastic
  - tabular representation
- forbidden area, we consider case 1
  - case 1: accesible with penalty
  - case 2: inaccessible
- state transition probability
- policy: tells the agent waht actions to take at a state
  - deterministic policy
  - stochastic policy, sum = 1
  - tabular representation
- reward
  - positive -> encouragement
  - negative -> punishment
  - zero -> neutral, no punishment
  - can positive mean punishment? Yes
  - Example: grid world
    - r_bound = -1
    - r_forbidden = -1
    - r_target =  +1
    - r = 0, otherwise
  - **Reward can be interpreted as a human-machine interface, with which can guide the agent to behave as what we expect.**
  - tabular representation
  - the reward depends on the state and action, but not the next state
- trajectory: state-action-reward chain
- return
- discounted return
- episode: a finite trajectory (trial)
- continuing tasks: no terminal states
  - option 1: treat target state as a special absorbing state
  - option 2: treat target state as a normal state
  - we consider option 2

### Markov Decision Process (MDP)

- Sets
  - State
  - Action
  - Reward
- Probability
  - State transition probability
  - Reward probability
- Policy: at state s, take action a
- Markov property: memoryless property
  - the future is independent of the past given the present
  - the state transition probability and reward probability depend only on the current state and action
- Markov decision process becomes Markov process once the policy is given!

## Bellman Equation
